{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import compgraph as cg\n",
    "from autodiff.reverse import gradient, check_gradient, visualize_AD\n",
    "\n",
    "import warnings\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Reverse AD\n",
    "## Single Path to a Variable\n",
    "\n",
    "$$\n",
    "f(x) = \\sin(2\\ln x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = cg.variable(2, name='x')\n",
    "ln = cg.log(x, name='ln')\n",
    "mul = cg.constant(2, name='2') * ln\n",
    "f = cg.sin(mul, name='sin')\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = cg.variable(2, name='x')\n",
    "y = 3 * x\n",
    "f = x * y\n",
    "g = cg.log(y)\n",
    "z = y + f + g\n",
    "print(z)\n",
    "print(gradient(z))\n",
    "\n",
    "visualize_AD(z, figsize=(11, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = cg.variable(2, name='x')\n",
    "y = cg.log(x)\n",
    "f = x * y\n",
    "g = cg.log(y)\n",
    "z = y + f + g\n",
    "print(z)\n",
    "print(gradient(z))\n",
    "\n",
    "visualize_AD(z, figsize=(11, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = cg.variable(4, name='x')\n",
    "two = cg.constant(2, name='2')\n",
    "f = x ** two + two ** x\n",
    "f.name = 'add'\n",
    "\n",
    "visualize_AD(f, figsize=(11, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-variable Functions\n",
    "\n",
    "$$\n",
    "f(x,y,z) = \\sin(x+y) + (xy)^z\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a function f:Rn→R, it takes only one application of reverse mode AD to compute the entire gradient\n",
    "x = cg.variable(1, name='x')\n",
    "y = cg.variable(2, name='y')\n",
    "z = cg.variable(3, name='z')\n",
    "\n",
    "add_0 = x + y\n",
    "add_0.name = 'add_0'\n",
    "mul = x*y\n",
    "mul.name = 'mul'\n",
    "powr = mul ** z\n",
    "powr.name = 'pow'\n",
    "sin = cg.sin(add_0)\n",
    "sin.name='sin'\n",
    "add_1 = sin + powr\n",
    "add_1.name='add_1'\n",
    "\n",
    "f2 = add_1\n",
    "\n",
    "visualize_AD(f2, figsize=(11, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Checking\n",
    "\n",
    "$$\n",
    "f(x, y, z) = \\sin\\left(x^{y + z}\\right) - 3z\\ln\\left(x^2y^3\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.433811705909566\n",
      "Gradient Checking Result: True\n",
      "\n",
      "∂f/∂x = 28.59729544270365\n",
      "∂f/∂y = 4.971684551677847\n",
      "∂f/∂z = -8.521081615041496\n"
     ]
    }
   ],
   "source": [
    "def func(x,y,z):\n",
    "    _x = cg.variable(x, 'x')\n",
    "    _y = cg.variable(y, 'y')\n",
    "    _z = cg.variable(z, 'z')\n",
    "\n",
    "    return  cg.sin(_x ** (_y + _z)) - 3 * _z * cg.log((_x ** 2) * (_y ** 3))\n",
    "\n",
    "f = func(0.5, 4, -2.3)\n",
    "print(f)\n",
    "g = gradient(f)\n",
    "\n",
    "print(\"Gradient Checking Result: {}\".format(check_gradient(func, [0.5, 4, -2.3], [g[v] for v in ['x', 'y', 'z']])))\n",
    "print(\"\")\n",
    "print(\"∂f/∂x = {}\".format(g['x']))\n",
    "print(\"∂f/∂y = {}\".format(g['y']))\n",
    "print(\"∂f/∂z = {}\".format(g['z']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Network on MNIST Dataset\n",
    "\n",
    "## Loading and Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "0\n",
      "4\n",
      "[0 0 0 0 0 1 0 0 0 0]\n",
      "[1 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 1 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
    "label_binarizer = LabelBinarizer()\n",
    "\n",
    "# transforming all geryscale values to range [0,1]\n",
    "# 0 being black and 1 beiung white \n",
    "X_scaled = X / 255\n",
    "\n",
    "for i in range(3):\n",
    "    print(y[i])\n",
    "# transfrom categorical target labels into one-vs-all fashion\n",
    "y_binarized = label_binarizer.fit_transform(y)\n",
    "for i in range(3):\n",
    "    print(y_binarized[i])\n",
    "\n",
    "# splitting the data to 80% training and 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_binarized, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining and Running the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss (Last 1k Iterations): 0.00007: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50000/50000 [03:32<00:00, 235.63it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm, trange\n",
    "\n",
    "import compgraph as cg\n",
    "from autodiff.reverse import gradient\n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "learning_rate = 0.01\n",
    "BATCH_SIZE = 32\n",
    "ITERATIONS = 50000\n",
    "\n",
    "def relu(x):\n",
    "    return cg.where(x > 0, x, 0)\n",
    "\n",
    "# He Initialization\n",
    "l1_weights = cg.variable(np.random.normal(scale=np.sqrt(2./784), size=(784, 64)), name='l1_w')\n",
    "l1_bias = cg.variable(np.zeros(64), name='l1_b')\n",
    "l2_weights = cg.variable(np.random.normal(scale=np.sqrt(2./64), size=(64, 10)), name='l2_w')\n",
    "l2_bias = cg.variable(np.zeros(10), name='l2_b')\n",
    "\n",
    "\n",
    "def nn(x):\n",
    "    l1_activations = relu(cg.dot(x, l1_weights) + l1_bias)\n",
    "    l2_activations = cg.dot(l1_activations, l2_weights) + l2_bias\n",
    "    \n",
    "    return l2_activations\n",
    "\n",
    "last1000_losses = []\n",
    "progress_bar = trange(ITERATIONS)\n",
    "training_set_pointer = 0\n",
    "\n",
    "for i in progress_bar:\n",
    "    batch_x = X_train[training_set_pointer:training_set_pointer + BATCH_SIZE]\n",
    "    batch_y = y_train[training_set_pointer:training_set_pointer + BATCH_SIZE]\n",
    "    \n",
    "    if training_set_pointer + BATCH_SIZE >= len(y_train):\n",
    "        # if the training set is consumed, start from the beginning\n",
    "        training_set_pointer = 0\n",
    "    else:\n",
    "        training_set_pointer += BATCH_SIZE\n",
    "    \n",
    "    logits = nn(batch_x)\n",
    "    loss = cg.softmax_cross_entropy(logits, batch_y)\n",
    "    last1000_losses.append(loss)\n",
    "    \n",
    "    progress_bar.set_description(\n",
    "        \"Avg. Loss (Last 1k Iterations): {:.5f}\".format(np.mean(last1000_losses))\n",
    "    )\n",
    "    \n",
    "    if len(last1000_losses) == 1000:\n",
    "        last1000_losses.pop(0)\n",
    "    \n",
    "    grads = gradient(loss)\n",
    "    \n",
    "    l1_weights -= learning_rate * grads['l1_w']\n",
    "    l2_weights -= learning_rate * grads['l2_w']\n",
    "    l1_bias -= learning_rate * grads['l1_b']\n",
    "    l2_bias -= learning_rate * grads['l2_b']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tesing the NN's Accuracy on Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.53%\n"
     ]
    }
   ],
   "source": [
    "def softmax(x, axis):\n",
    "    x_max = cg.max(x, axis=axis, keepdims=True)\n",
    "    exp_op = cg.exp(x - x_max)\n",
    "    return exp_op/ cg.sum(exp_op, axis=axis, keepdims=True)\n",
    "\n",
    "logits = nn(X_test)\n",
    "probabilities = softmax(logits, axis=-1)\n",
    "predicted_labels = np.argmax(probabilities, axis=-1)\n",
    "true_labels = np.argmax(y_test, axis=-1)\n",
    "accuracy = np.mean(predicted_labels == true_labels)\n",
    "\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
